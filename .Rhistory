knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(keras)
#### WORKARROUND para que TF funcione en GPU, comentar las lineas si se actualiza
# y se resuelve el bug https://github.com/tensorflow/tensorflow/issues/43174
library(tensorflow)
physical_devices <- tf$config$list_physical_devices('GPU')
tf$config$experimental$set_memory_growth(physical_devices[[1]], TRUE)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(keras)
#### WORKARROUND para que TF funcione en GPU, comentar las lineas si se actualiza
# y se resuelve el bug https://github.com/tensorflow/tensorflow/issues/43174
library(tensorflow)
physical_devices <- tf$config$list_physical_devices('GPU')
tf$config$experimental$set_memory_growth(physical_devices[[1]], TRUE)
physical_devices <- tf$config$list_physical_devices('GPU')
physical_devices <- tf$config$list_physical_devices('GPU')
#### WORKARROUND para que TF funcione en GPU, comentar las lineas si se actualiza
# y se resuelve el bug https://github.com/tensorflow/tensorflow/issues/43174
library(tensorflow)
physical_devices <- tf$config$list_physical_devices('GPU')
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(keras)
#### WORKARROUND para que TF funcione en GPU, comentar las lineas si se actualiza
# y se resuelve el bug https://github.com/tensorflow/tensorflow/issues/43174
library(tensorflow)
physical_devices <- tf$config$list_physical_devices('GPU')
tf$config$experimental$set_memory_growth(physical_devices[[1]], TRUE)
physical_devices <- tf$config$list_physical_devices('GPU')
# Cambiar valores de los parámetros
mlflow_run(entry_point = "mnist-cnn_mlflow.R", parameters = list(dropout = 0.5, epochs = 3))
library(mlflow)
# Lanzar script de entrenamiento
mlflow_run(entry_point = "mnist-cnn_mlflow.R")
## -------------------------------------------------------------------------------------
## Sistemas Inteligentes para la Gestión en la Empresa
## Curso 2020-2021
## Juan Gómez Romero
## -------------------------------------------------------------------------------------
library(keras)
if( ! ("mlflow" %in%  installed.packages()[,"Package"]) ) {
install.packages("mlflow")
library(mlflow)
mlflow::install_mlflow()
}
library(mlflow)
# Lanzar script de entrenamiento
mlflow_run(entry_point = "mnist-cnn_mlflow.R")
## -------------------------------------------------------------------------------------
## Sistemas Inteligentes para la Gestión en la Empresa
## Curso 2020-2021
## Juan Gómez Romero
## -------------------------------------------------------------------------------------
library(reticulate)
use_condaenv('r-tensorflow')
## -------------------------------------------------------------------------------------
## Sistemas Inteligentes para la Gestión en la Empresa
## Curso 2020-2021
## Juan Gómez Romero
## -------------------------------------------------------------------------------------
library(reticulate)
use_condaenv('r-tensorflow')
library(keras)
if( ! ("mlflow" %in%  installed.packages()[,"Package"]) ) {
install.packages("mlflow")
library(mlflow)
mlflow::install_mlflow()
}
library(mlflow)
# Lanzar script de entrenamiento
mlflow_run(entry_point = "mnist-cnn_mlflow.R")
mlflow::install_mlflow()
# Lanzar script de entrenamiento
mlflow_run(entry_point = "mnist-cnn_mlflow.R")
# Visualizar en interfaz MLflow
# http://127.0.0.1:5987/
mlflow_ui()
install.packages("mlflow")
install.packages("mlflow")
library(mlflow)
mlflow::install_mlflow()
# Visualizar en interfaz MLflow
# http://127.0.0.1:5987/
mlflow_ui()
setwd("~/MEGA/universidad/SIGE/p2")
source('~/MEGA/universidad/SIGE/sige2021/teoría/tema 4/mnist/mnist-cnn_mlflow.R')
# Lanzar script de entrenamiento
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R")
library(reticulate)
use_condaenv('r-tensorflow')
library(keras)
if( ! ("mlflow" %in%  installed.packages()[,"Package"]) ) {
install.packages("mlflow")
library(mlflow)
mlflow::install_mlflow()
}
library(mlflow)
# Lanzar script de entrenamiento
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R")
# Lanzar script de entrenamiento
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R")
# Lanzar script de entrenamiento
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R")
# Lanzar script de entrenamiento
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R")
hidden_units      <- mlflow_param("hidden_units", 100, "integer", "Number of units of the hidden layer")
hidden_activation <- mlflow_param("hidden_activation", "sigmoid", "string", "Activation function for the hidden layer")
dropout_rate      <- mlflow_param("dropout", 0.3, "numeric", "Dropout rate (after the hidden layer)")
epsilon           <- mlflow_param("epsilon", 0.01, "numeric", "Epsilon parameter of the batch normalization (after convolution)")
batch_size        <- mlflow_param("batch_size", 128, "integer", "Mini-batch size")
epochs            <- mlflow_param("epochs", 5, "integer", "Number of training epochs")
# Directorios
dataset_dir           <- './datasets/medium10000_twoClasses/'
train_images_dir      <- paste0(dataset_dir, 'train')
val_images_dir        <- paste0(dataset_dir, 'val')
test_images_dir       <- paste0(dataset_dir, 'test')
# Generadores
train_images_generator <- image_data_generator(rescale = 1/255)
val_images_generator   <- image_data_generator(rescale = 1/255)
test_images_generator  <- image_data_generator(rescale = 1/255)
validation_generator_flow <- flow_images_from_directory(
directory = val_images_dir,
generator = val_images_generator,
class_mode = 'categorical',
batch_size = 5,
target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
# Definir arquitectura
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = hidden_activation, input_shape = c(64, 64, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = hidden_activation) %>% layer_batch_normalization(epsilon = epsilon) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = hidden_activation) %>% layer_batch_normalization(epsilon = epsilon) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = hidden_activation) %>% layer_batch_normalization(epsilon = epsilon) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 512, activation = hidden_activation) %>%
layer_dropout(rate = dropout_rate)
# Flujos
train_generator_flow <- flow_images_from_directory(
directory = train_images_dir,
generator = train_images_generator,
class_mode = 'categorical',
batch_size = 5,
target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
test_generator_flow <- flow_images_from_directory(
directory = test_images_dir,
generator = test_images_generator,
class_mode = 'categorical',
batch_size = 5,
target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
summary(model)
# Compilar modelo
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Entrenar modelo
history <- model %>%
fit_generator(
generator = train_generator_flow,
validation_data = validation_generator_flow,
steps_per_epoch = batch_size,
epochs = epochs
)
#### WORKARROUND para que TF funcione en GPU, comentar las lineas si se actualiza
# y se resuelve el bug https://github.com/tensorflow/tensorflow/issues/43174
library(tensorflow)
physical_devices <- tf$config$list_physical_devices('GPU')
tf$config$experimental$set_memory_growth(physical_devices[[1]], TRUE)
library(reticulate)
use_condaenv('r-tensorflow')
library(keras)
#### WORKARROUND para que TF funcione en GPU, comentar las lineas si se actualiza
# y se resuelve el bug https://github.com/tensorflow/tensorflow/issues/43174
library(tensorflow)
physical_devices <- tf$config$list_physical_devices('GPU')
tf$config$experimental$set_memory_growth(physical_devices[[1]], TRUE)
####
library(mlflow)
hidden_units      <- mlflow_param("hidden_units", 100, "integer", "Number of units of the hidden layer")
hidden_activation <- mlflow_param("hidden_activation", "sigmoid", "string", "Activation function for the hidden layer")
dropout_rate      <- mlflow_param("dropout", 0.3, "numeric", "Dropout rate (after the hidden layer)")
epsilon           <- mlflow_param("epsilon", 0.01, "numeric", "Epsilon parameter of the batch normalization (after convolution)")
batch_size        <- mlflow_param("batch_size", 128, "integer", "Mini-batch size")
epochs            <- mlflow_param("epochs", 5, "integer", "Number of training epochs")
# Directorios
dataset_dir           <- './datasets/medium10000_twoClasses/'
train_images_dir      <- paste0(dataset_dir, 'train')
val_images_dir        <- paste0(dataset_dir, 'val')
test_images_dir       <- paste0(dataset_dir, 'test')
# Generadores
train_images_generator <- image_data_generator(rescale = 1/255)
val_images_generator   <- image_data_generator(rescale = 1/255)
test_images_generator  <- image_data_generator(rescale = 1/255)
# Flujos
train_generator_flow <- flow_images_from_directory(
directory = train_images_dir,
generator = train_images_generator,
class_mode = 'categorical',
batch_size = 5,
target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
validation_generator_flow <- flow_images_from_directory(
directory = val_images_dir,
generator = val_images_generator,
class_mode = 'categorical',
batch_size = 5,
target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
test_generator_flow <- flow_images_from_directory(
directory = test_images_dir,
generator = test_images_generator,
class_mode = 'categorical',
batch_size = 5,
target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
# Definir arquitectura
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = hidden_activation, input_shape = c(64, 64, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = hidden_activation) %>% layer_batch_normalization(epsilon = epsilon) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = hidden_activation) %>% layer_batch_normalization(epsilon = epsilon) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = hidden_activation) %>% layer_batch_normalization(epsilon = epsilon) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 512, activation = hidden_activation) %>%
layer_dropout(rate = dropout_rate)
summary(model)
# Compilar modelo
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Entrenar modelo
history <- model %>%
fit_generator(
generator = train_generator_flow,
validation_data = validation_generator_flow,
steps_per_epoch = batch_size,
epochs = epochs
)
# Definir arquitectura
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = hidden_activation, input_shape = c(64, 64, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = hidden_activation) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = hidden_activation) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = hidden_activation) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 512, activation = hidden_activation) %>%
layer_dropout(rate = dropout_rate)
# Compilar modelo
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Entrenar modelo
history <- model %>%
fit_generator(
generator = train_generator_flow,
validation_data = validation_generator_flow,
steps_per_epoch = batch_size,
epochs = epochs
)
# Definir arquitectura
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = hidden_activation, input_shape = c(64, 64, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = hidden_activation) %>% layer_batch_normalization(epsilon = epsilon) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = hidden_activation) %>% layer_batch_normalization(epsilon = epsilon) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = hidden_activation) %>% layer_batch_normalization(epsilon = epsilon) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = hidden_units, activation = hidden_activation) %>%
layer_dropout(rate = dropout_rate) %>%
layer_dense(units = 2, activation = "softmax")
summary(model)
# Compilar modelo
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# Entrenar modelo
history <- model %>%
fit_generator(
generator = train_generator_flow,
validation_data = validation_generator_flow,
steps_per_epoch = batch_size,
epochs = epochs
)
## -------------------------------------------------------------------------------------
## Sistemas Inteligentes para la Gestión en la Empresa
## Curso 2020-2021
## Juan Gómez Romero
## -------------------------------------------------------------------------------------
library(reticulate)
use_condaenv('r-tensorflow')
library(keras)
if( ! ("mlflow" %in%  installed.packages()[,"Package"]) ) {
install.packages("mlflow")
library(mlflow)
mlflow::install_mlflow()
}
library(mlflow)
# Lanzar script de entrenamiento
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R")
# Visualizar en interfaz MLflow
# http://127.0.0.1:5987/
mlflow_ui()
# Lanzar script de entrenamiento
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R")
# Guardar valores interesantes de la ejecución
# Por ejemplo, para estudio de dropout + epochs
mlflow_log_param("dropout_rate", dropout_rate)
mlflow_log_param("epochs", epochs)
# Guardar modelo
mlflow_log_model(model, "model")
# Calcular metricas sobre datos de validación
metrics <- model %>%
evaluate_generator(test_generator_flow, steps = 1)
# Guardar valores interesantes de la ejecución
# Por ejemplo, para estudio de dropout + epochs
mlflow_log_param("dropout_rate", dropout_rate)
mlflow_log_param("epochs", epochs)
mlflow_log_metric("loss", metrics["loss"])
mlflow_log_metric("accuracy", metrics["accuracy"])
# Guardar modelo
mlflow_log_model(model, "model")
# Mostrar salida
message("CNN model (dropout=", dropout, ", epochs=", epochs, "):")
# Mostrar salida
message("CNN model (dropout=", dropout_rate, ", epochs=", epochs, "):")
message("  loss: ", metrics["loss"])
message("  accuracy: ", metrics["accuracy"])
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(keras)
#### WORKARROUND para que TF funcione en GPU, comentar las lineas si se actualiza
# y se resuelve el bug https://github.com/tensorflow/tensorflow/issues/43174
library(tensorflow)
physical_devices <- tf$config$list_physical_devices('GPU')
tf$config$experimental$set_memory_growth(physical_devices[[1]], TRUE)
####
library(reticulate)
use_condaenv('r-tensorflow')
# install.packages("remotes")
# remotes::install_github("rstudio/reticulate")
predictions <- predict_generator(model, test_generator_flow, steps = 10)
y_true <- test_generator_flow$classes
y_pred <- ifelse(predictions[,1] > 0.55, 1, 0)
y_pred_factor <- factor(y_pred, levels=c('0','1'))
library(caret)
cm <- confusionMatrix(factor(y_pred, levels = c("0","1")),as.factor(y_true))
cm <- confusionMatrix(factor(y_pred, levels = c("0","1")),as.factor(y_true))
cm_prop <- prop.table(cm$table)
cm <- confusionMatrix(y_pred_factor,as.factor(y_true))
predictions <- predict_generator(model, test_generator_flow, steps = 10)
y_pred <- ifelse(predictions[,1] > 0.55, 1, 0)
y_pred
library(reticulate)
use_condaenv('r-tensorflow')
library(keras)
if( ! ("mlflow" %in%  installed.packages()[,"Package"]) ) {
install.packages("mlflow")
library(mlflow)
mlflow::install_mlflow()
}
library(mlflow)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(reticulate)
use_condaenv('r-tensorflow')
library(keras)
#### WORKARROUND para que TF funcione en GPU, comentar las lineas si se actualiza
# y se resuelve el bug https://github.com/tensorflow/tensorflow/issues/43174
library(tensorflow)
physical_devices <- tf$config$list_physical_devices('GPU')
tf$config$experimental$set_memory_growth(physical_devices[[1]], TRUE)
####
dataset_dir           <- './datasets/medium10000_twoClasses/'
train_images_dir      <- paste0(dataset_dir, 'train')
val_images_dir        <- paste0(dataset_dir, 'val')
test_images_dir       <- paste0(dataset_dir, 'test')
# https://tensorflow.rstudio.com/keras/reference/image_data_generator.html
train_images_generator <- image_data_generator(rescale = 1/255)
val_images_generator   <- image_data_generator(rescale = 1/255)
test_images_generator  <- image_data_generator(rescale = 1/255)
# https://tensorflow.rstudio.com/keras/reference/flow_images_from_directory.html
# https://forums.fast.ai/t/split-data-using-fit-generator/4380/4
batch_size <- 64
train_generator_flow <- flow_images_from_directory(
directory = train_images_dir,
generator = train_images_generator,
class_mode = 'categorical',
batch_size = batch_size,
target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
validation_generator_flow <- flow_images_from_directory(
directory = val_images_dir,
generator = val_images_generator,
class_mode = 'categorical',
batch_size = batch_size,
target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
test_generator_flow <- flow_images_from_directory(
directory = test_images_dir,
generator = test_images_generator,
class_mode = 'categorical',
batch_size = batch_size,
target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
metadata_train <- read_tsv(paste0(train_images_dir, "/multimodal_train.tsv"))
metadata_train <- metadata_train %>%
mutate(created_at = as.POSIXct(created_utc, origin="1970-01-01")) %>%
mutate(class = ifelse(`2_way_label` == 0, 'Disinformation', 'Other')) %>%
select(-all_of(c('created_utc', '6_way_label', '3_way_label', '2_way_label' )))
summary(metadata_train)
library(funModeling)
df_status(metadata_train)
table(metadata_train$class)
ggplot(metadata_train) +
geom_histogram(aes(x = class, fill = as.factor(class)), stat = "count") +
labs(x = "", y = "")
plotdata <- metadata_train %>%
filter(class == "Disinformation") %>%
count(subreddit) %>%
slice_max(n = 15, order_by = n, with_ties = FALSE)
ggplot(plotdata) +
geom_bar(aes(x = subreddit, y = n), stat = 'identity') +
coord_flip()
library(scales)
ggplot(metadata_train, aes(x = created_at)) +
geom_histogram(aes(fill = class))
library(mice)
data_binary_factors <- metadata_train %>%
select(-one_of("author", "clean_title", "id", "image_url", "linked_submission_id", "title", 'domain')) %>%
mutate_if(is.character, as.factor)
imputation <- mice(data_binary_factors, defaultMethod = c('mean', 'logreg', 'polyreg', 'polr'))
train <- complete(imputation) %>% na.omit()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = "relu", input_shape = c(64, 64, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 512, activation = "relu") %>%
layer_dense(units = 2, activation = "softmax")
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>%
fit_generator(
generator = train_generator_flow,
validation_data = validation_generator_flow,
steps_per_epoch = train_generator_flow$samples / batch_size,
epochs = 5
)
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout = 0.5, epochs = 15, batch_size=256))
## -------------------------------------------------------------------------------------
## Sistemas Inteligentes para la Gestión en la Empresa
## Curso 2020-2021
## Juan Gómez Romero
## Adaptado al problema de fakenews por Francisco José González García
## -------------------------------------------------------------------------------------
library(reticulate)
use_condaenv('r-tensorflow')
library(keras)
library(mlflow)
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout = 0.5, epochs = 15, batch_size=256))
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout = 0.5, epochs = 15, batch_size=256))
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout = 0.5, epochs = 15, batch_size=128))
library(reticulate)
use_condaenv('r-tensorflow')
library(keras)
if( ! ("mlflow" %in%  installed.packages()[,"Package"]) ) {
install.packages("mlflow")
library(mlflow)
mlflow::install_mlflow()
}
library(mlflow)
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout = 0.5, epochs = 15, batch_size=128))
# Visualizar en interfaz MLflow
# http://127.0.0.1:5987/
mlflow_ui()
# Visualizar entrenamiento
plot(history)
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout_rate = 0.3, epochs = 15, batch_size=128))
library(reticulate)
use_condaenv('r-tensorflow')
library(keras)
if( ! ("mlflow" %in%  installed.packages()[,"Package"]) ) {
install.packages("mlflow")
library(mlflow)
mlflow::install_mlflow()
}
library(mlflow)
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout_rate = 0.3, epochs = 15, batch_size=128))
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout_rate = 0.3, epochs = 15, batch_size=100))
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout_rate = 0.3, epochs = 15, batch_size=64))
# Visualizar en interfaz MLflow
# http://127.0.0.1:5987/
mlflow_ui()
mlflow_run(entry_point = "clasificacion_binaria_mlflow.R", parameters = list(dropout_rate = 0.3, epochs = 15))
